
\subsection{Performance de MPTCP sur mininet}
\label{sec:CR:perfMPTCP:base}

Après la compilation du noyau, pour vérifier le fonctionnement de
MPTCP nous avons mesurer le débit moyen en utilisant \emph{iperf} sur
la topologie A.

Les paramètres\footnote{paramètres par défaut dans le noyau Linux}
utilisés sont les suivants:


\vspace{1cm}
%\begin{tabular}{lp{\linewidth - 4cm}} 
\begin{tabular}{ll}
\textbf{Paramètre}& \textbf{Valeur}\\
\hline
MSS& 1460 octets\\
window size& 85,3 Koctets\\
délai par lien & 10 ms\\
Algorithme de congestion& LIA \cite{rfc6356}\\
\end{tabular}
\vspace{0.5cm}

Si nous fixons le MSS à 1460 octets, nous obtenons ce message
d'erreur:
\begin{verbatim}
  WARNING: attempt to set TCP maximum segment size to 1460, but got
  536
\end{verbatim}

Cependant, si nous analysons les paquets enregistrés grâce à TCPdump,
nous observons que la taille maximal du MSS négocié pendant le
\emph{handshake} de la connexion MPTCP est de 1460 octets et que la
taille du MSS dans les paquets de données est de 1428 octets.

\subsubsection{Un exemple de démonstration}
\label{sec:CR:perfMPTCP:unique}

Ici nous allons prendre l'exemple d'une connexion avec deux sous-flots
à 100 Mbit/s. Voici la commande pour générer cet exemple:
\begin{verbatim}
sudo python ./pyMPTCP -O exp001_TC --bw 100 -t 30 -n 2 --mptcp --bwm_ng
\end{verbatim}

Pour les détails de l'activation de MPTCP dans le noyau et
l'utilisation des arguments des scripts python, une notice est donnée
en Annexe 1: voir sections \ref{sec:annexe1:usepyth} page
\pageref{sec:annexe1:usepyth} et \ref{sec:annexe1:mininetParserargs}
page \pageref{sec:annexe1:mininetParserargs}.

\vspace{0.5cm}
Le délai (RTT) entre h1 et sv\_mp1 est de 44\,$\pm$\,11\,ms (mesuré
avec la commande ping), ce qui correspond bien au délai nécessaire
pour traverser deux liens. La moyenne ici tient compte du délai du
premier paquet qui est envoyé vers le contrôleur qui va établir le
chemin vers le serveur\footnote{Il pourrait être nécessaire d'enlever
  le délai de ce premier paquet dans une version future.}.

L'argument ``-\,-bwm-ng'' permet de lancer Bandwidth Monitor
NG\footnote{\url{http://www.gropp.org/?id=projects&sub=bwm-ng} }
(bwm-ng) et de mesurer plusieurs paramètres comme le nombre d'octets,
de paquets, le débit entrant ou sortant passant par chacune des
interfaces sondées. La Figure \ref{fig:MPTCP-perfbwm-ng} montre le
débit entrant côté serveur pour ses deux interfaces, nous observons
que pour deux sous-flots aux capacités identiques, le débit mesuré est
quasi identique (une différence de quelques paquets est observée).

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/bw-single.pdf}
    \caption{\textbf{Débit entrant côté serveur.} échantillonage : 2\,Hz.}
  \label{fig:MPTCP-perfbwm-ng}
\end{figure}

Pour mesurer le débit total généré, nous moyennons les débits totaux
mesurés toutes les secondes par \emph{iperf} entre 5 secondes après le
début de la connexion et 1 seconde avant la fin de la connexion. Nous
obtenons, côté serveur et pour cet exemple, un débit maximal de
168\,Mbit/s ce qui est attendu par les mesures de débit \emph{via}
bwm\_ng.


\subsubsection{Variation du débit maximal par lien}
\label{sec:CR:perfMPTCP:nsousflots}

Pour connaître les limites de la capacité de notre simulation, nous
faisons varier le débit maximal par sous-flot, ainsi que le nombre de
sous-flots dans la topologie A.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/bw-coupled.pdf}
    \caption{\textbf{Débit total mesuré en fonction du débit maximal
        par sous-flot.} Les débits sont mesurés avec \emph{iperf} pour
      une connexion TCP classique et une connexion MPTCP contenant de
      2 à 6 sous-flots.}
  \label{fig:MPTCP-perf-subflow-bw}
\end{figure}

Nous observons une phase linéaire où l'augmentation du débit par lien
ou l'augmentation du nombre de sous-flots augmente linéairement le
débit total obtenu côté serveur. Cette phase située en dessous des
100\,Mbit/s par lien correspond au but de MPTCP : augmentation du
débit. Cependant, les performances décroissent rapidement et tombent
sous les performances d'une simple connexion TCP ce qui est contraire
à la nature même de MPTCP \cite{rfc6182}.

Ce problème pourrait être expliquer par l'utilisation non optimale de
la capacité des sous-flots. Le \emph{bandwidth delay product} (BDP)
implique une taille minimale du tampon de réception. Pour un débit de
1000\,Mo et un RTT de 44\,ms, on obtient une taille minimale de tampon
de 5,5\,Mo. Sachant que le tampon de réception est partagé pour tous
les sous-flots d'une connexion MPTCP \cite{rfc6824}, la taille
minimale du tampon de réception doit suivre cette formule :

\begin{equation}
  \label{eq:MPTCP:buffer}
  buffer\_size \geqslant max(\{RTT_i\}_{i \in [1,n]})*\sum_{i \in [1,n]} Bandwidth_{\{i\}}
\end{equation}

C'est à dire que la taille du tampon de réception doit être le produit
du RTT maximal parmi tous les sous-flots et le débit total de tous les
sous-flots réunis. Cette taille de tampon garantit l'utilisation
optimale du lien lorsque des paquets nécessitent d'être retransmisent
sur des sous-flots aux délais lents. Dans notre simulation, il n'y a
pas de perte de paquet, la valeur minimale correspond au BDP le plus
élevé.

Pour les mêmes propriétés de liens, avec deux sous-flots, nous
obtenons une taille minimale de 11\,Mo. Mininet modife automatiquement
les tampons au lancement de la topologie et les valeurs utilisées sont
les suivantes:

\begin{verbatim}
net.core.wmem_max = 16777216
net.core.wmem_default = 163840
net.core.rmem_max = 16777216
net.core.rmem_default = 163840
net.ipv4.tcp_wmem = 10240	87380	16777216
net.ipv4.tcp_rmem = 10240	87380	16777216
net.ipv4.tcp_mem = 19326	25768	38652
\end{verbatim}

Nous observons que le tampon maximal pouvant être alloué par socket
est de 16\,Mo environ ce qui est largement supérieur à la taille
requise. De plus en augmentant la taille du tampon, nous n'observons
pas d'augmentation de performances alors qu'en le diminuant, nous
observons une diminution du débit mesuré
Fig. \ref{fig:mptcp:windowscale}.

\begin{figure}[!htb]
    \includegraphics[width=0.7\textwidth]{../figures/ws.pdf}
  
  \caption{\textbf{Débit mesuré en fonction de la taille de la
      fenêtre}. La taille maximmale de la fenêtre TCP est modifiée
    avec l'argument '-w' avec \emph{iperf}. La taille obtenue est
    étrangement de deux fois supérieure à la taille demandé. Le nombre
    de paquets correspond à la taille maximale de la queue des
    routeurs.}
  \label{fig:mptcp:windowscale}
  
\end{figure}

Nous obtenons les mêmes résultats en modifiant la taille maximale de
la queue des routeurs ou en modifiant les valeurs dans le noyau (voir
\ref{sec:annexe1:windowsize} page \pageref{sec:annexe1:windowsize})
que ce soit les paramètres minimum, par défaut et maxium
d'\emph{auto-tuning} de TCP (\emph{net.ipv4}) ou les valeurs maximales
ou par défaut pour tous les type de connexions (\emph{net.core}). Une
vérification de la charge CPU global avec \emph{htop} ne montre pas
une saturation des processeurs (environ 15\,\% d'utilisation)
cependant, il reste à implémenter \emph{cpuacct} pour vérifier la
charge CPU par conteneur.


En effectuant des
recherches\footnote{\url{https://github.com/mininet/mininet/wiki/Introduction-to-Mininet\#what-are-mininets-limitations}}\footnote{\url{https://mailman.stanford.edu/pipermail/mininet-discuss/2014-January/003901.html}},
il semblerait que la limite du débit est lié aux n\oe uds Open vSwitch
sur Ubuntu 13.04, ce qui pour une dizaine de liens limite la capacité
à 100 Mbit/s. C'est pourquoi, nous limiterons le débit maximal par
lien à environ 10 Mbit/s.

\subsubsection{Variation du délai par lien}
\label{sec:compterendu:perf:délai}

Afin d'évaluer l'influence du délai sur le débit enregistré, nous
faisons varier le délai de tous les liens de manière symétriques. Le
coût pour chaque sous-flot reste donc identique.

Pour mesurer le délai pour chaque sous-flot, nous avons utilisé la
commande \emph{ping}. Cette approche a été utilisé car elle est la
plus facile à mettre en \oe uvre. Par manque d'espace disque (mémoire
SSD), l'utilisation de \emph{tcpdump} est réservée pour les tests et
la recherche d'erreur. Cependant, il sera nécessaire d'utiliser le
délai par TCP car l'utilisation conjointe de \emph{ping} et
d'\emph{iperf} produit une erreur (voir
\pref{sec:annexe1:bugs:mininet}).\\


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/rtt.pdf}
  \centering
  
  \caption{\textbf{Débit maximal mesuré en fonction du délai}. Le
    délai est du même ordre de grandeur pour tous les liens. Le RTT
    est mesuré avec la commande \emph{ping} avant le test avec
    \emph{iperf} qui dure 60\,s. Deux sous-flots sont utilisés.}
  \label{fig:mptcp:rttisotrop}
  
\end{figure}

Nous observons que la modification du délai n'entraîne pas de baisse
de performances pour les liens à faible débits
\fig{fig:mptcp:rttisotrop}. Ce qui est attendu vu que la taille des
\emph{buffers} et la taille maximale de paquets attendant sur les
routeurs sont largement suffisantes pour pallier à une augmentation de
débit. Il reste alors à tester ces cas dans des
conditions plus stringentes.\\

Pour les liens à plus haut débits, l'augmentation de délai entraîne
une diminution du débit. Pour 200\,Mbit/s et 400\,ms de RTT, il est
nécessaire d'avoir 10\,Mo environ de tampon par sous-flot. Cependant,
il est possible que cette diminution drastique pour le lien à
200\,Mbit/s est liée aux problèmes de simulation des débits élévés
avec mininet.\\
\vspace{1cm}

Nous constatons que la taille du tampon s'avère être un paramètre
primordial dans pour les performances de MPTCP. L'algorithme de
congestion utilisé ne semble pas être nécessaire dans ces cas
particuliers. Dans une utilisation mobile, les deux sous-flots ont
généralement des délais différents si on se base sur l'utilisation
d'un accès wifi, 3G ou/et ethernet. Dans la figure
\ref{fig:mptcp:rttetTC}, nous utilisons les mêmes paramètres que dans
l'expérience précédente cependant le premier sous-flot a un délai
(RTT) fixe d'une dizaine de millisecondes (expériences ``TC''). Nous
observons aucune différence entre le cas où les deux sous-flots ont un
délai variable et le cas où un seul sous-flot est variable sauf dans
l'expérience ``200 TC'' où le débit est plus élevé que dans le cas à
deux sous-flots variables ce qui confirme en partie le fait que la
diminution de performance observée, pour ce cas, est lié à la taille
du tampon.


\begin{figure}[htb]
    \includegraphics[width=0.7\textwidth]{../figures/prefixtc.pdf}
  \centering
  \caption{\textbf{Débit maximal mesuré en fonction du délai d'un lien}. Le
    délai est modifiée pour le second lien dans la topologie A. Le RTT
    est mesuré avec la commande \emph{ping} avant le test avec
    \emph{iperf} qui dure 60\,s. Deux sous-flots sont utilisés.}
  \label{fig:mptcp:rttetTC}
\end{figure}

\subsubsection{Choix du sous-flot en fonction du délai}
\label{sec:compterendu:perf:5choix}


Pour tester les sous-flots utilisés dans une connexion à \og faible
\fg débit. Nous avons utilisé 5 sous-flots à délai variable. Le délai
de chaque sous-flot suit une sigmoïde (équation \ref{eq:sigmoide}).

\begin{equation}
  \label{eq:sigmoide}
  delay=min+\frac{max}{1+e^{\frac{x-x_{half}}{slope}}}
\end{equation}

Les paramètres\footnote{slope et $x_{half}$ ne sont pas utiles et
  permettent de contraindre le nombre d'expériences et la courbure des
  points d'inflexion.} utilisés sont les suivants:

\vspace{1cm}
%\begin{tabular}{lp{\linewidth - 4cm}} 
\begin{tabular}{ll}
\textbf{Paramètre}& \textbf{Valeur}\\
\hline
delay& délai par lien\\
max& délai maximal par lien\\
min& délai minimal par lien\\
\end{tabular}
\vspace{0.5cm}


\begin{figure}[htb]
    \includegraphics[width=0.7\textwidth]{../figures/TC5.pdf}
  \centering
  \caption{\textbf{Débit reçu par interface côté serveur en fonction
      du RTT par sous-flot}. \emph{En haut}, RTT pour chaque
    sous-flot. \emph{Au milieu}, débit (Mbit/s) par interface. En bas,
    débit total (Mbit/s) enregistré au serveur.}
  \label{fig:mptcp:sigmoid}
\end{figure}

Nous observons que le débit reçu est équivalent pour chaque interface
sauf pour l'interface ``eth4'' où son débit est inférieur à celles des
autres pour des RTTs supérieur à 300\,ms. Le débit total n'est que
faiblement diminué. En effet, la commande \emph{iperf} envoie des
paquets jusqu'à atteindre le débit maximum sur chaque sous-flot. La
légère diminution des débits pour les délais long est probablement
liée au tampon de réception ou d'envoi.\\

Il est donc difficile de prédire les sous-flots préférés par
l'ordonnanceur. \emph{iperf} ne permet de contraindre le débit que
pour des paquets UDP. Nous avons donc intégré
\emph{iperf3\footnote{\url{https://github.com/esnet/iperf}}} à la VM
mininet
pour permettre de fixer le débit.\\

L'utilisation d'\emph{iperf3} entraîne une erreur qui n'a pas été
encore résolue à la fin de la simulation (voir
\pref{sec:annexe1:bugs:mininet}) empêchant toutes simulations
automatisées. Nous avons donc choisi deux résultats caractéristiques.

\begin{figure}[!htb]
    \includegraphics[width=0.7\textwidth]{../figures/bw-5_1_5_10_100_500.pdf}
  \centering
  \caption{\textbf{Débit pour chaque interface}. Les RTTs pour les
    interfaces de eth0 à eth5 sont de 34, 38,43, 137, 552\,ms. Chaque
    lien a une capacité de 10\,Mbit/s et le débit total généré par
    \emph{iperf3} a été fixé à 7,5\,Mbit/s.  }
  \label{fig:mptcp:bw151050}
\end{figure}

Dans la figure \ref{fig:mptcp:bw151050} et
\ref{fig:mptcp:bw1010500500500} nous observons que l'ordonnanceur
envoie les paquets préférentiellement dans les sous-flots à délai
court. Il équilibre les charges dans les sous-flots où le délai reste
raisonnable (sous-flot à RTT de 147\,ms dans la figure
\ref{fig:mptcp:bw151050}. Cependant pour les sous-flots à \og coûts
\fg identiques, nous observons un effet de battement
(\emph{flappiness}) qui peut être conséquent. Il serait utile de
mesurer le RTT au cours du temps pour chaque sous-flot et d'établir si
cet effet de battement est lié à une variation du délai et ce pour
l'algorithme LIA et OLIA \cite{pareto2013}.

\begin{figure}[!htb]
    \includegraphics[width=0.7\textwidth]{../figures/bw-5_10_10_500_500_500.pdf}
  \centering
  \caption{\textbf{Débit pour chaque interface}. Les RTTs pour les
    interfaces de eth0 à eth5 sont de 43, 43, 557, 557,
    557\,ms. Chaque lien a une capacité de 10\,Mbit/s et le débit
    total généré par \emph{iperf3} a été fixé à 7,5\,Mbit/s.  }
  \label{fig:mptcp:bw1010500500500}
\end{figure}




\subsubsection{Choix de l'algorithme}
\label{sec:compterendu:perf:algo}

La plupart des tests ont aussi été effectués avec l'algorithme de
congestion de linux (\emph{cubic}) et certains avec l'algorithme
\emph{OLIA}. L'utilisation de ces algorithmes entraînent des résultats
grossièrement similaires cependant, des analyses plus poussées et des
tests dans des conditions de plus grand stress devront être effectués.
