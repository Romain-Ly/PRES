
\subsection{Performance de MPTCP sur mininet}
\label{sec:CR:perfMPTCP:base}

Après la compilation du noyau, pour vérifier le fonctionnement de
MPTCP nous avons mesuré le débit moyen en utilisant \emph{iperf} sur
la topologie A.

Les paramètres\footnote{paramètres par défaut dans le noyau Linux}
utilisés sont les suivants:


\vspace{1cm}
%\begin{tabular}{lp{\linewidth - 4cm}} 
\begin{tabular}{ll}
  \textbf{Paramètre}& \textbf{Valeur}\\
  \hline
  MSS& 1460 octets\footnote{Une erreur se produit lorsque nous fixons la valeur voir \pref{sec:annexe1:bugs:mininet}}\\
  window size& 85,3 Koctets\\
  délai par lien & 10 ms\\
  Algorithme de congestion& LIA \cite{rfc6356}\\
\end{tabular}
\vspace{0.5cm}


\subsubsection{Un exemple de démonstration}
\label{sec:CR:perfMPTCP:unique}

Nous allons prendre, dans cet exemple, une connexion avec deux
sous-flots avec une capacité individuelle de 100 Mbit/s. Voici la
commande pour générer cet exemple:
\begin{verbatim}
sudo python ./pyMPTCP -O exp001_TC --bw 100 -t 30 -n 2 --mptcp --bwm_ng
\end{verbatim}

Pour les détails de l'activation de MPTCP dans le noyau et
l'utilisation des arguments des scripts python, une notice est donnée
en Annexe 1: voir sections \ref{sec:annexe1:usepyth} page
\pageref{sec:annexe1:usepyth} et \ref{sec:annexe1:mininetParserargs}
page \pageref{sec:annexe1:mininetParserargs}.

\vspace{0.5cm} Le RTT entre h1 et sv\_mp1 est de 44\,$\pm$\,11\,ms
(mesuré avec la commande \emph{ping}), ce qui correspond au RTT
attendu pour traverser deux liens aller-retour. La moyenne ici tient
compte du RTT du premier paquet qui est envoyé vers le contrôleur pour
que celui-ci établisse le chemin vers le serveur.

L'argument ``-\,-bwm\_ng'' permet de lancer Bandwidth Monitor
NG\footnote{\url{http://www.gropp.org/?id=projects&sub=bwm-ng} }
(bwm-ng). Cette application mesure plusieurs paramètres: le nombre
d'octets, de paquets, le débit entrant ou sortant passant par chacune
des interfaces de l'hôte sondé. La figure \ref{fig:MPTCP-perfbwm-ng}
représente le débi entrant enregistré au serveur pour ses deux
interfaces. Nous observons pour deux sous-flots aux capacités
identiques et au coût identique, que le débit mesuré est quasi
similaire (une différence de quelques paquets est observée).

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/bw-single.pdf}
    \caption{\textbf{Débit entrant côté serveur.} échantillonage : 2\,Hz.}
  \label{fig:MPTCP-perfbwm-ng}
\end{figure}

Pour mesurer le débit total généré, nous effectuons une moyenne des
débits totaux mesurés toutes les secondes par \emph{iperf} entre 5
secondes après le début de la connexion, et 1 seconde avant la fin de
la connexion. Nous observons pour cet exemple, côté serveur, un débit
maximal de 168\,Mbit/s. Ce débit est cohérent par rapport aux mesures
de débit \emph{via} bwm-ng.


\subsubsection{Variation du débit maximal par lien}
\label{sec:CR:perfMPTCP:nsousflots}

Pour connaître les limites de nos simulations, nous avons fait varier
la capacité de chaque sous-flot, ainsi que le nombre de sous-flots
dans la topologie A.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/bw-coupled.pdf}
    \caption{\textbf{Débit total mesuré en fonction de la capacité du
        sous-flot.} Les débits sont mesurés avec \emph{iperf} pour une
      connexion TCP classique et une connexion MPTCP contenant de 2 à
      6 sous-flots.}
  \label{fig:MPTCP-perf-subflow-bw}
\end{figure}

Le débit totale mesuré au serveur croît linéairement avec
l'augmentation du nombre de sous-flots pour des capacités par lien de
10 à environ 100\,Mbit/s. Cette phase est en adéquation avec le but de
MPTCP : augmentation des performances \cite{rfc6182}. Cependant, pour
les liens aux capacités supérieures à 100\,Mbit/s, les performances
décroissent rapidement et tombent en dessous des performances d'une
simple connexion TCP ce qui viole la nature même de MPTCP .

Ce problème pourrait être expliqué par une utilisation non optimale
de la capacité des sous-flots. Le \emph{bandwidth delay product} (BDP)
implique une taille minimale du tampon de réception. Pour un débit de
1000\,Mo et un RTT de 44\,ms, la taille minimale du tampon est de
5,5\,Mo. Sachant que le tampon de réception\footnote{dans notre
  simulation la taille des tampons de réception est la même que la
  taille des tampons d'envoi.}  est partagé pour tous les sous-flots
d'une connexion MPTCP, la taille minimale du tampon doit suivre cette
formule \cite{rfc6824} :

\begin{equation}
  \label{eq:MPTCP:buffer}
  buffer\_size \geqslant max(\{RTT_i\}_{i \in [1,n]})*\sum_{i \in [1,n]} Bandwidth_{\{i\}}
\end{equation}

C'est à dire que la taille du tampon de réception doit être supérieure
ou égal au produit du RTT le plus élevé parmi tous les sous-flots et
la somme des capacités de tous les sous-flots. Cette taille de tampon
garantit l'utilisation optimale du lien lorsque des paquets
nécessitent d'être retransmisent sur des sous-flots aux délais
lents. Dans notre simulation, il n'y a pas de perte de paquets, la
valeur minimale des tampons correspond au BDP le plus élevé.

Pour les mêmes propriétés de liens, avec deux sous-flots, nous
obtenons une taille minimale de 11\,Mo. Mininet modife automatiquement
les tampons au lancement de la topologie et les valeurs utilisées sont
les suivantes (en octets):

\begin{verbatim}
net.core.wmem_max = 16777216
net.core.wmem_default = 163840
net.core.rmem_max = 16777216
net.core.rmem_default = 163840
net.ipv4.tcp_wmem = 10240	87380	16777216
net.ipv4.tcp_rmem = 10240	87380	16777216
net.ipv4.tcp_mem = 19326	25768	38652
\end{verbatim}

Nous observons que le tampon maximal pouvant être alloué par socket
est de 16\,Mo environ ce qui est largement supérieur à la taille
requise. De plus en augmentant la taille du tampon, nous n'observons
pas d'augmentation de performances alors qu'en le diminuant, nous
observons une diminution du débit mesuré (voir
Fig. \ref{fig:mptcp:windowscale}).

\begin{figure}[!htb]
    \includegraphics[width=0.7\textwidth]{../figures/ws.pdf}
    \centering
    \caption{\textbf{Débit mesuré en fonction de la taille de la
        fenêtre}. La taille maximmale de la fenêtre TCP est modifiée
      avec l'argument '-w' avec \emph{iperf}. La taille obtenue est
      étrangement deux fois supérieure à la taille demandé. Le nombre
      de paquets indiqué dans la légende correspond à la capacité de
      traitement, en nombre de paquets, des routeurs.}
  \label{fig:mptcp:windowscale}
  
\end{figure}

Nous obtenons les mêmes résultats en modifiant la capacité des
routeurs ou en modifiant les valeurs de la taille des tampons dans le
noyau (voir \ref{sec:annexe1:windowsize} page
\pageref{sec:annexe1:windowsize}) que ce soit les paramètres minimum,
par défaut et maxium d'\emph{auto-tuning} de TCP (\emph{net.ipv4}) ou
les valeurs maximales ou par défaut pour tous les type de connexions
(\emph{net.core}). Une vérification de la charge CPU global avec
\emph{htop} ne montre pas une saturation des processeurs (environ
15\,\% d'utilisation) cependant, il reste à implémenter \emph{cpuacct}
pour vérifier la charge CPU par conteneur.


En effectuant des
recherches\footnote{\url{https://github.com/mininet/mininet/wiki/Introduction-to-Mininet\#what-are-mininets-limitations}}\footnote{\url{https://mailman.stanford.edu/pipermail/mininet-discuss/2014-January/003901.html}},
il semblerait que la limite du débit est lié aux n\oe uds Open vSwitch
sur Ubuntu 13.04, ce qui pour une dizaine de liens, limite la capacité
maximale à environ 100 Mbit/s. De plus, il semblerait que l'activation
de \emph{auto-tuning} pour les tailles des tampons entraînent une
basse significative des performances de MPTCP \cite{PKB13}. C'est pourquoi,
nous limiterons la capacité des liens à de faibles valeurs.

\subsubsection{Variation du délai par lien}
\label{sec:compterendu:perf:délai}

Afin d'évaluer l'influence du délai sur le débit enregistré, le délai
de tous les liens varie de manière symétrique. Le coût pour chaque
sous-flot reste donc identique.

Pour mesurer le délai de chaque sous-flot, nous avons utilisé la
commande \emph{ping}. Cette approche a été utilisée car elle est la
plus facile à mettre en \oe uvre. Par manque d'espace disque (mémoire
SSD), l'utilisation de \emph{tcpdump} est réservée pour les tests et
la recherche d'erreur. Cependant, il sera nécessaire d'utiliser le
délai avec TCP car cette méthode est plus fiable et l'utilisation
conjointe de \emph{ping} et d'\emph{iperf} produit une erreur (voir
\pref{sec:annexe1:bugs:mininet}).\\


\begin{figure}[tb]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/rtt.pdf}
  \centering
  
  \caption{\textbf{Débit maximal mesuré en fonction du délai}. Le
    délai est du même ordre de grandeur pour tous les liens. Le RTT
    est mesuré avec la commande \emph{ping} avant le test avec
    \emph{iperf} qui dure 60\,s. Deux sous-flots sont utilisés.}
  \label{fig:mptcp:rttisotrop}
  
\end{figure}

Nous observons que la modification du délai n'entraîne pas de
diminution des performances pour les liens à faibles capacités
\fig{fig:mptcp:rttisotrop}. Ce qui est attendu vu que la taille des
\emph{buffers} et la capacité des routeurs sont largement suffisantes
pour satisfaire à une augmentation de débit. Il reste alors à tester
ces cas dans des
conditions plus stringentes.\\

Pour les liens à haute capacité, l'augmentation de délai entraîne une
diminution du débit total. Pour 200\,Mbit/s et 400\,ms de RTT, il est
nécessaire d'avoir environ 10\,Mo de tampon par sous-flot. Cependant,
il est possible que cette diminution drastique pour le lien à
200\,Mbit/s est en partie liée aux problèmes de simulation des débits
élévés
avec mininet.\\


Nous constatons que la taille du tampon s'avère être un paramètre
primordial dans les performances de MPTCP. Dans une utilisation
mobile, les deux sous-flots ont généralement des délais différents
(par exemple, si on se base sur l'utilisation conjointe d'un accès
wifi, 3G ou/et ethernet). Dans la figure \ref{fig:mptcp:rttetTC}, nous
utilisons les mêmes paramètres que dans l'expérience précédente
cependant le premier sous-flot aura le même RTT (une dizaine de
millisecondes) pour toutes les expériences (expériences ``TC''). Nous
n'observons aucune différence notable entre le cas où les deux
sous-flots ont un RTT variable et dans le cas où un seul des
sous-flots possède un RTT variable. L'expérience ``200 TC'' montre que
le débit est plus élevé que l'expérience ``200'', cette différence
peut être expliquer par une taille du tampon insuffisante.



\begin{figure}[htb]
    \includegraphics[width=0.7\textwidth]{../figures/prefixtc.pdf}
  \centering
  \caption{\textbf{Débit total mesuré en fonction du délai d'un
      lien}. Le délai est modifiée seulement pour le second lien dans
    la topologie A (expériences ``TC''). Le RTT est mesuré avec la
    commande \emph{ping} avant le test avec \emph{iperf} qui dure
    60\,s. Deux sous-flots sont utilisés.}
  \label{fig:mptcp:rttetTC}
\end{figure}

\subsubsection{Choix du sous-flot en fonction du délai}
\label{sec:compterendu:perf:5choix}


Pour tester quels sont les sous-flots choisies par MPTCP dans une
connexion à \og faible \fg débit. Nous avons utilisé 5 sous-flots avec
différents RTT. Le RTT de chaque sous-flot suit une sigmoïde (équation
\ref{eq:sigmoide}).

\begin{equation}
  \label{eq:sigmoide}
  delay=min+\frac{max}{1+e^{\frac{x-x_{half}}{slope}}}
\end{equation}

Les paramètres\footnote{slope et $x_{half}$ ne sont pas utiles et
  permettent de contraindre le nombre d'expériences et la courbure des
  points d'inflexion.} utilisés sont les suivants:

\vspace{1cm}
%\begin{tabular}{lp{\linewidth - 4cm}} 
\begin{tabular}{ll}
\textbf{Paramètre}& \textbf{Valeur}\\
\hline
delay& délai par lien\\
max& délai maximal par lien\\
min& délai minimal par lien\\
\end{tabular}
\vspace{0.5cm}

\begin{figure}[H]
    \includegraphics[width=0.7\textwidth]{../figures/TC5.pdf}
  \centering
  \caption{\textbf{Débit reçu par interface côté serveur en fonction
      du RTT par sous-flot}. \emph{En haut}, RTT pour chaque
    sous-flot. \emph{Au milieu}, débit (Mbit/s) par interface. En bas,
    débit total (Mbit/s) enregistré au serveur.}
  \label{fig:mptcp:sigmoid}
\end{figure}

Nous observons que le débit reçu est globalement équivalent pour
chaque interface sauf pour l'interface ``eth4'' où son débit est
inférieur à celles des autres pour des RTTs supérieur à 300\,ms. Le
débit total n'est que faiblement diminué. En effet, la commande
\emph{iperf} envoie des paquets jusqu'à atteindre la capacité de
chaque sous-flot. La légère diminution des débits pour les délais long
est probablement
liée au tampon de réception ou d'envoi.\\

Il est donc difficile de prédire les sous-flots utilisés par
l'ordonnanceur. \emph{iperf} ne permet de contraindre le débit que
pour des paquets UDP. Nous avons donc intégré
\emph{iperf3\footnote{\url{https://github.com/esnet/iperf}}} à la VM
mininet
pour permettre de fixer le débit pour des paquets TCP.\\

L'utilisation d'\emph{iperf3} entraîne une erreur qui n'a pas été
encore résolue à la fin de la simulation (voir
\pref{sec:annexe1:bugs:mininet}) empêchant toutes simulations
automatisées. Nous avons donc choisi deux résultats caractéristiques.

\begin{figure}[H]
    \includegraphics[width=0.7\textwidth]{../figures/bw-5_1_5_10_100_500.pdf}
  \centering
  \caption{\textbf{Débit mesuré pour chaque interface}. Les RTTs pour
    les interfaces de eth0 à eth5 sont de 34, 38, 43, 137,
    552\,ms. Chaque lien a une capacité de 10\,Mbit/s et le débit
    total généré par \emph{iperf3} a été fixé à 7,5\,Mbit/s.  }
  \label{fig:mptcp:bw151050}
\end{figure}

Dans la figure \ref{fig:mptcp:bw151050} et
\ref{fig:mptcp:bw1010500500500} nous observons que MPTCP envoie
préférentiellement les paquets dans les sous-flots à faible RTT. Il
équilibre les charges dans les sous-flots où le délai reste
raisonnable (sous-flot à RTT de 147\,ms dans la figure
\ref{fig:mptcp:bw151050}. Cependant pour les sous-flots à \og coûts
\fg identiques, nous observons un effet de battement
(\emph{flappiness}). Il serait utile de mesurer le RTT au cours du
temps pour chaque sous-flot et d'établir si cet effet de battement est
lié à une variation du délai et ce pour l'algorithme LIA et OLIA
\cite{pareto2013}.

\begin{figure}[H]
    \includegraphics[width=0.7\textwidth]{../figures/bw-5_10_10_500_500_500.pdf}
  \centering
  \caption{\textbf{Débit mesuré pour chaque interface}. Les RTTs pour
    les interfaces de eth0 à eth5 sont de 43, 43, 557, 557,
    557\,ms. Chaque lien a une capacité de 10\,Mbit/s et le débit
    total généré par \emph{iperf3} a été fixé à 7,5\,Mbit/s.  }
  \label{fig:mptcp:bw1010500500500}
\end{figure}



