
\subsection[Outil de coordination: git]{Outil de coordination: git}
\label{subsec:conception2:git}
L'état des scripts utilisés par l'équipe est mise à jour par un
système de version utilisant git
\url{https://github.com/Romain-Ly/PRES}. Le noyau contenant les
modifications de MPTCP est disponible ici :
\url{https://github.com/Finaler/mptcp/tree/mptcp_pres_test1}

Ce système permet de distribuer l'état de scripts à l'ensemble de
l'équipe afin d'être opérationnel sur leurs sous-projets respectifs.

\subsection{Mininet}
\label{sec:conce:Mininet}

Nous avons utilisé trois langages pour les scripts.

\paragraph{python}
Dans un premier temps, nous avons utilisé python pour pouvoir intégrer
les fonctions de l'API python et remplir les fonctions suivantes:
\begin{itemize}
\item créer des topologies
\item créer des expériences variées
\item intégrer des outils pour la mesure des performances
\item intégrer un parseur d'argument pour automatiser les procédures
\end{itemize}

Le but des scripts est de pouvoir intégrer des topologies, des
expériences à la volée et de permettre d'écrire des fichiers de
sorties différenciés afin de rendre l'analyse plus facile. Une
expérience peut se définir comme l'ensemble des contraintes apportés à
la topologie (délai d'un ou de plusieurs liens, tests utilisées,
déroulement des mesures).

Une description plus détaillée est effectuée dans le compte-rendu (voir
\pref{sec:compterendu}) et dans l'annexe (voir \pref{sec:annexe1:annexe1}).

\paragraph{bash}
Pour multiplier les expériences, nous utilisons des scripts
\emph{bash} exécutant les scripts pythons en série.

\paragraph{R}
Pour analyser les fichiers de sortie (iperf, bwm-ng, ping, ...) et
pour les figures nous avons utilisé R\cite{Rcite}.

\subsection{Performances de MPTCP}
\label{sec:conce:perfMPTCP}
Pour mesurer les performances de MPTCP, nous avons utilisé des outils
disponibles sur linux comme \emph{iperf, iperf3, bwm\_ng et ping}.


\subsection{Conception algorithme sécurisé}
\label{sec:conc:algosec}

Après l'étude du code de MPTCP, nous avons étudié et discuté sur la
manière d'obtenir un rendement maximal dans l'implémentation de
l'ordonnanceur. Les discussions n'ont pas abouti sur un consensus,
nous avons décidé dans un premier temps de coder une version de
l'ordonnanceur chacun de notre côté.


Ce développpement concurrentiel a permis de développer des idées
différentes et de permettre de les confronter directement dans le
noyau. Au final, nous avons gardé la version la mieux structurée qui
était la plus apte aux modifications et à un développement à deux.
\\

Par la suite, nous avons corrigé les nombreuses erreurs dans le code.
Une fois que notre code était compilé, nous avons testé si les tâches
demandées étaient réalisées selon nos souhaits. Pour cela, nous nous
sommes partagé le travail en testant le code chacun de notre côté et
en utilisant différents moyens afin de comprendre ce qui ne marchait
pas dans notre algorithme.
\\

Afin de ne pas perdre de temps et de rester le plus productif
possible, nous partagions les erreurs et les problèmes rencontrés avec
les autres membres du groupe afin de savoir si quelqu'un avait déjà eu
ce problème ou pour nous donner une idée de solution.
